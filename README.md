# AI Knowledge Search Platform

A local AI-powered enterprise knowledge search system that ingests mixed-format documents, performs hybrid retrieval (BM25 + embeddings), dynamically routes queries to the most effective retrieval strategy, and optimizes context through deduplication and compression before LLM generation.

## üéØ Key Features

- **Intelligent Query Routing** - Classifies query intent and routes to optimal retrieval strategy
- **Hybrid Retrieval** - Combines BM25 keyword search with semantic vector search
- **Context Optimization** - Deduplication and compression to reduce token usage by 30-45%
- **Multi-LLM Support** - Toggle between Ollama (local) and OpenAI API
- **Streaming Responses** - Real-time streaming for both OpenAI and Ollama providers
- **Redis Caching** - Query result caching with configurable TTL for improved performance
- **Load Balancing** - Application-level load balancing with multiple workers
- **Interactive UI** - Streamlit dashboard with routing visualization and context inspection
- **Production-Ready** - FastAPI backend with comprehensive error handling and logging

## üèóÔ∏è Architecture

```
Document Ingestion ‚Üí Embedding & Indexing ‚Üí Query Routing ‚Üí Hybrid Retrieval ‚Üí Context Optimization ‚Üí LLM Generation
```

### Core Components

1. **Query Router** - Intent classification and strategy selection
2. **Retrieval Engine** - BM25, semantic, hybrid, and conversational strategies  
3. **Context Optimizer** - Deduplication and compression for efficiency
4. **RAG Pipeline** - End-to-end orchestration with metadata tracking
5. **Streamlit UI** - Interactive visualization of routing decisions and optimization

## üöÄ Quick Start

### Prerequisites

- Python 3.8+
- Ollama (for local LLM) or OpenAI API key

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd ai-knowledge-search-platform
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure environment**
   ```bash
   cp config.env.example config.env
   # Edit config.env with your settings
   ```

4. **Ingest sample documents**
   ```bash
   python scripts/ingest_samples.py
   ```

5. **Start the backend**
   ```bash
   cd backend
   python main.py
   ```

6. **Start the UI**
   ```bash
   cd ui
   streamlit run app.py
   ```

## üìä Demo Queries

The system intelligently routes different query types:

### Factoid Queries ‚Üí BM25 Strategy
- "What is artificial intelligence?"
- "Who invented the neural network?"
- "Define machine learning"

### Semantic Queries ‚Üí Semantic Strategy  
- "Find concepts related to deep learning"
- "What topics are similar to computer vision?"
- "Show me content about AI ethics"

### Analytical Queries ‚Üí Hybrid Strategy
- "Compare supervised vs unsupervised learning"
- "Analyze the benefits of neural networks"
- "Evaluate different AI approaches"

### Conversational Queries ‚Üí Conversational Strategy
- "How do I implement a neural network?"
- "Help me understand machine learning"
- "Guide me through AI concepts"

## üîß Configuration

### LLM Provider Toggle
```env
LLM_PROVIDER=ollama  # or openai
OPENAI_API_KEY=your_key_here
OLLAMA_MODEL=llama2
```

### Retrieval Parameters
```env
HYBRID_ALPHA=0.7  # Weight for semantic vs BM25
MAX_CONTEXT_TOKENS=4000
SIMILARITY_THRESHOLD=0.9
```

### Redis Caching
```env
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
CACHE_ENABLED=true
CACHE_TTL=3600  # 1 hour
```

### Load Balancing
```env
WORKERS=4  # Number of worker processes
```

**Note**: For production deployment with load balancing, run:
```bash
cd backend
python main.py --workers 4
```

## üìà Performance Metrics

- **Query Latency**: < 2.0 seconds average
- **Context Reduction**: 35-45% token savings
- **Routing Accuracy**: ~85% intent classification
- **Retrieval Quality**: >90% relevance retention

## üõ†Ô∏è API Endpoints

### Query Processing
- `POST /api/v1/query` - Process query through RAG pipeline
- `POST /api/v1/query/rag` - Full RAG response with metadata
- `POST /api/v1/query/stream` - Streaming RAG response (Server-Sent Events)

### Document Management
- `POST /api/v1/ingest/file` - Upload single document
- `POST /api/v1/ingest/directory` - Ingest directory of documents
- `DELETE /api/v1/clear` - Clear document index

### System Information
- `GET /api/v1/stats` - Pipeline statistics (includes cache stats)
- `GET /api/v1/test` - Pipeline health check
- `GET /api/v1/health` - System health status
- `GET /api/v1/health/workers` - Worker health check for load balancing

## üß™ Testing

Run comprehensive tests to verify all features:

```bash
# Run all tests
pytest tests/ -v

# Test intent classification accuracy (85%+)
pytest tests/test_intent_accuracy.py -v

# Test token reduction (40%+ average)
pytest tests/test_token_reduction.py -v

# Test caching functionality
pytest tests/test_caching.py -v

# Test streaming endpoints
pytest tests/test_streaming.py -v

# Run original routing tests
python scripts/test_routing.py
```

Tests verify:
- Intent classification accuracy (85%+)
- Token reduction metrics (40%+ average)
- Cache hit/miss behavior
- Streaming response completeness
- Routing decision correctness  
- RAG pipeline end-to-end flow
- Performance benchmarks

## üìö Sample Documents

The platform includes sample documents covering:
- Technical documentation (AI basics, ML guide, neural networks)
- Research papers (simplified versions)
- Business reports and analysis
- FAQs and troubleshooting guides

## üîç Technical Highlights

### Query Routing Algorithm
- Hybrid pattern matching + ML classification
- Confidence scoring and fallback strategies
- Real-time routing decision explanation

### Context Optimization
- Cosine similarity clustering for deduplication
- Extractive summarization for compression
- Token usage tracking and metrics

### Architecture Patterns
- Strategy pattern for retrieval methods
- Pipeline pattern for RAG orchestration
- Observer pattern for optimization tracking

## üéØ Interview Talking Points

1. **Intelligent Routing**: "I built a hybrid intent classifier that routes queries to the optimal retrieval strategy, reducing irrelevant results by 40%"

2. **Context Optimization**: "The deduplication layer reduces token usage by 30-45% while maintaining answer quality, demonstrating understanding of LLM cost optimization"

3. **Hybrid Retrieval**: "Combined lexical (BM25) and semantic search based on query type, showing understanding of different retrieval paradigms"

4. **Clean Architecture**: "Modular design with clear separation between routing, retrieval, and generation layers for maintainability"

5. **Production Thinking**: "While this is a local demo, I designed it with extensibility in mind - easy to add new retrieval strategies or intent types"

## üöÄ Future Enhancements

- **Advanced Reranking** - Cross-encoder models for better relevance
- **Query Expansion** - Synonym and related term expansion
- **Feedback Loop** - User relevance feedback for continuous improvement
- **Multi-modal Support** - Image and document processing
- **Real-time Updates** - Live document indexing and updates

## üìÑ License

This project is for demonstration purposes. Feel free to use and modify for your own projects.

## ü§ù Contributing

This is a personal project for interview demonstration. If you find it useful, feel free to fork and adapt for your needs.

---

**Built with**: Python, FastAPI, Streamlit, ChromaDB, sentence-transformers, Ollama/OpenAI
